# NOTE: Explicit dependencies need to be set for items otherwise creation will fail. 
provider "aws" {
    region = "${var.k8scfg["parm_region"]}"
	version = "~> 1.6"
}

//terraform { backend "s3" {} }
variable "gpolicy_arn" {
    type = "list"
    description = "List of group policy arns needed by the Kubernetes group"
    default = [
        "arn:aws:iam::aws:policy/AmazonEC2FullAccess",
        "arn:aws:iam::aws:policy/AmazonRoute53FullAccess",
        "arn:aws:iam::aws:policy/AmazonS3FullAccess",
        "arn:aws:iam::aws:policy/IAMFullAccess",
        "arn:aws:iam::aws:policy/AmazonVPCFullAccess",
    ]
}
resource "aws_iam_group" "group" {
    depends_on = [ "aws_iam_user.user" ]
    name = "${var.k8scfg["parm_group"]}"
    path = "/"
}
resource "aws_iam_group_policy_attachment" "gpa" {
    depends_on = [ "aws_iam_group.group" ]
    count = "${length(var.gpolicy_arn)}"
    group = "${var.k8scfg["parm_group"]}"
    policy_arn = "${element(var.gpolicy_arn, count.index)}"
}
resource "aws_iam_user" "user" {
    name = "${var.k8scfg["parm_user"]}"
    path = "/"
    force_destroy = "${var.k8scfg["md_force_destroy"]}"
}
resource "aws_iam_group_membership" "gm" {
    depends_on = [ "aws_iam_group.group", "aws_iam_user.user" ]
    name = "${var.k8scfg["parm_group"]}"
    group = "${var.k8scfg["parm_group"]}"
    users = [
        "${var.k8scfg["parm_user"]}",
    ]
}
resource "aws_iam_access_key" "cak" {
    depends_on = [ 
        "aws_iam_user.user", 
    ]
    user = "${var.k8scfg["parm_user"]}"
}

data "aws_route53_zone" "hz" {
    name = "${var.k8scfg["parm_domain"]}"
}
resource "aws_route53_zone" "subhz" {
    depends_on = [ "aws_iam_user.user" ] 
    name = "${var.k8scfg["parm_subdomain"]}.${var.k8scfg["parm_domain"]}"
    comment = "${var.k8scfg["parm_comment"]}"
    tags {
        Name = "${var.k8scfg["tags_project"]}-subhz"
        Project = "${var.k8scfg["tags_project"]}"
        Provider = "${var.k8scfg["tags_provider"]}"
    }
}
resource "aws_route53_record" "subhz_nsrecords" {
    depends_on = [ "aws_iam_user.user" ] 
    zone_id = "${data.aws_route53_zone.hz.zone_id}"
    name    = "${var.k8scfg["parm_subdomain"]}.${var.k8scfg["parm_domain"]}"
    type    = "NS"
    ttl     = "300"
    records = ["${values(data.external.subhz_nsrecords.result)}"]
}
data "external" "subhz_nsrecords" {
    program = [ "/bin/bash", "nsrecords.sh" ] 
    query = {
        hosted_zone = "${aws_route53_zone.subhz.id}"
    }
}
resource "aws_s3_bucket" "s3b" {
    depends_on = [ "aws_iam_user.user" ] 
    bucket = "${replace(var.k8scfg["parm_domain"],"/\\..*/","")}-${var.k8scfg["tags_project"]}-state"
    acl    = "private"
    force_destroy = "true"
    region = "${var.k8scfg["parm_region"]}"
    tags {
        Name = "${var.k8scfg["tags_project"]}-s3b"
        Project = "${var.k8scfg["tags_project"]}"
        Provider = "${var.k8scfg["tags_provider"]}"
    }
    versioning {
        enabled = "${var.k8scfg["parm_versioning"]}"
    }
}
# Sets up AWS configuration files and the kops Access_key and Secret Access
# Access key and secret are generated by AWS when the "kops" user is created. 
# These will be used for running kops. 
module "awscfg" {
    source = "../modules/awscfg"
    cfg = {
        id = "${aws_iam_access_key.cak.id}"
        secret = "${aws_iam_access_key.cak.secret}"
        user = "${var.k8scfg["parm_user"]}"
        profile = "${var.k8scfg["parm_user"]}"
    }
}

resource "null_resource" "kops" {
    depends_on = [
        "aws_iam_user.user",
        "aws_iam_access_key.cak",
        "aws_s3_bucket.s3b",
        "aws_route53_record.subhz_nsrecords",    
    ]
    triggers {
        ak_id = "${aws_iam_access_key.cak.id}"
        s3b_id = "${aws_s3_bucket.s3b.id}"
        subhz_id = "${aws_route53_zone.subhz.zone_id}"
    }
    
    provisioner "local-exec" {
        when = "create"
        # Using heredoc syntax for running multiple cmds
        command = <<CMD
wget -O kops https://github.com/kubernetes/kops/releases/download/$(curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d '"' -f 4)/kops-linux-amd64
chmod +x ./kops
sudo mv ./kops /usr/local/bin/
CMD
        interpreter = [ "/bin/bash", "-c" ] 
    }

    # Uninstall kops on destroy 
    provisioner "local-exec" {
        when = "destroy"
        command = "KOPS=$(which kops); if [[ -e \"$KOPS\" ]]; then sudo rm $KOPS; fi"
        interpreter = [ "/bin/bash", "-c" ]
    }
}

resource "null_resource" "kubectl" {
    depends_on = [
        "aws_iam_user.user",
        "null_resource.kops",
    ]
    triggers {
        ak_id = "${aws_iam_access_key.cak.id}"
        s3b_id = "${aws_s3_bucket.s3b.id}"
        subhz_id = "${aws_route53_zone.subhz.zone_id}"
    }
    
    provisioner "local-exec" {
        when = "create"
        # Using heredoc syntax for running multiple cmds
        command = <<CMD
wget -O kubectl https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl
chmod +x ./kubectl
sudo mv ./kubectl /usr/local/bin/
CMD
        interpreter = [ "/bin/bash", "-c" ] 
    }

    # Uninstall kops on destroy 
    provisioner "local-exec" {
        when = "destroy"
        command = "KUBECTL=$(which kubectl); if [[ -e \"$KUBECTL\" ]]; then sudo rm $KUBECTL; fi"
        interpreter = [ "/bin/bash", "-c" ]
    }
}

/*
resource "null_resource" "k8scluster" {
    depends_on = [
        "aws_iam_user.user",
        "null_resource.kops",
    ]
    triggers {
        k8sc_name = "${var.k8scfg["parm_subdomain"]}.${var.k8scfg["parm_domain"]}"
        k8sc_s3b_name = "${aws_s3_bucket.s3b.id}"
        k8sc_ak_id = "${aws_iam_access_key.cak.id}"
        k8sc_ak_secret = "${aws_iam_access_key.cak.secret}"
        k8sc_ak_user = "${aws_iam_access_key.cak.user}"
    }
    
    provisioner "local-exec" {
        when = "create"
        # Using heredoc syntax for running multiple cmds
        # First we copy the export commands for the env variables for use by kops into the .bashrc file. 
        # Then we copy the export commands for the env variables for the AWS kops user account. into the .bashrc file.  
        # Then we create a public key for ssh access by kops to the various kops systems. 
        # admin is the user name required for Debian. (Seems like kops defaults to using debian for its cluster machines)
        #   
        command = <<CMD
if [[ ! -e ~/.bashrc ]]; then touch ~/.bashrc; fi
echo -e "export NAME=${null_resource.k8scluster.triggers.k8sc_name}" >> ~/.bashrc
echo -e "export KOPS_STATE_STORE=${null_resource.k8scluster.triggers.k8sc_s3b_name}" >> ~/.bashrc
echo -e "export AWS_PROFILE=${null_resource.k8scluster.triggers.k8sc_ak_user}" >> ~/.bashrc 
echo -e "export AWS_ACCESS_KEY_ID=${null_resource.k8scluster.triggers.k8sc_ak_id}" >> ~/,bashrc
echo -e "export AWS_SECRET_ACCESS_KEY=${null_resource.k8scluster.triggers.k8sc_ak_secret}" >> ~/.bashrc
ssh-keygen -t rsa -N "" -f ~/.ssh/id_rsa; if (($?)); then exit 1; fi
kops create secret --name ${null_resource.k8scluster.triggers.k8sc_name} sshpublickey admin -i ~/.ssh/id_rsa.pub; if (($?)); then exit 1; fi
kops create cluster \
--name=${null_resource.k8scluster.triggers.k8sc_name} \
--state=${null_resource.k8scluster.triggers.k8sc_s3b_name} \
--zones=${var.k8scfg.parm_region}a \
--node-count=${var.k8scfg.parm_nodes} \
--node-size=${var.k8scfg.parm_nodetype} \
--master-size=${var.k8scfg.parm_mastertype} \
--dns-zone=${null_resource.k8scluster.triggers.k8sc_name}
if (($?)); then exit 1; fi
CMD
        interpreter = [ "/bin/bash", "-c" ] 
    }

    # Unset the exported variables on destroy 
    provisioner "local-exec" {
        when = "destroy"
        command = <<CMD
unset NAME; unset KOPS_STATE_STORE

CMD
        interpreter = [ "/bin/bash", "-c" ]
    }
}
*/
output "k8scfg" { 
    value = {
        g_name = "${aws_iam_group.group.name}" 
        g_uid = "${aws_iam_group.group.unique_id}"
        g_arn = "${aws_iam_group.group.arn}"
        u_name = "${aws_iam_user.user.name}" 
        u_uid = "${aws_iam_user.user.unique_id}"
        u_arn = "${aws_iam_user.user.arn}"
        ak_id = "${aws_iam_access_key.cak.id}"
        ak_user = "${aws_iam_access_key.cak.user}"
        ak_secret = "${aws_iam_access_key.cak.secret}"
        hz_id = "${data.aws_route53_zone.hz.zone_id}"    
        subhz_id = "${aws_route53_zone.subhz.zone_id}"
        s3b_id = "${aws_s3_bucket.s3b.id}"
        s3b_arn = "${aws_s3_bucket.s3b.arn}"
        s3b_region = "${aws_s3_bucket.s3b.region}"
    }
}

output "subhz_records" {
    value = "${data.external.subhz_nsrecords.result}"
}
output "hz" {
    value = {
        caller_reference = "${data.aws_route53_zone.hz.caller_reference}"
        comment = "${data.aws_route53_zone.hz.comment}"
        #name_servers = "${data.aws_route53_zone.hz.*.name_servers}"
        resource_record_set_count = "${data.aws_route53_zone.hz.resource_record_set_count}"      
    } 
}
